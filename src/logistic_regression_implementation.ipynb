{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Input and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many types of owls are there?\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "import numpy as np\n",
    "import csv\n",
    "from decimal import Decimal\n",
    "import random as rnd\n",
    "import pandas as pd\n",
    "numpy.seterr(all='ignore')\n",
    "\n",
    "# import csv to visualise data\n",
    "df = pd.read_csv('../data/owls-csv.csv')\n",
    "\n",
    "# get the user input\n",
    "number_of_classes = int(input('How many types of owls are there?\\n'));\n",
    "##################################### UTILITY FUNCTIONS ######################################\n",
    "\n",
    "# file_reader \n",
    "#\n",
    "# Reads in the data file from local storage\n",
    "#\n",
    "# @params {String} file_name - location of file on disk\n",
    "def file_reader(file_name):\n",
    "    inputs = [];\n",
    "    i = 0;\n",
    "    with open(file_name, 'r') as csvfile:\n",
    "        file = csv.reader(csvfile, delimiter='\\n')\n",
    "        for row in file:\n",
    "            split_row = row[0].split(',')\n",
    "            floats = [float(x) for x in split_row]\n",
    "            inputs.append(floats)\n",
    "    # return the input array\n",
    "    return inputs;\n",
    "\n",
    "\n",
    "# normalise_data\n",
    "#\n",
    "# Normalises the data inputs for the model to erradicate the effect of outliers.\n",
    "# The data is normalised with 0-1 normalisation by dividing the data for each \n",
    "# instance by the max value for that instance.\n",
    "#\n",
    "# @params {List} data - list of lists, containing the data about each instance\n",
    "def normalise_data(data):\n",
    "    # number of intances in dataset\n",
    "    numberOfInstances = len(data)\n",
    "    # get min/max values from the dataset\n",
    "    max_values = map(max, zip(*data));\n",
    "    \n",
    "    max_values_array = [];\n",
    "    for val in max_values:\n",
    "        max_values_array.append(val);\n",
    "    max_values_array = [float(i) for i in max_values_array]\n",
    "    \n",
    "    for i in range(0, numberOfInstances):\n",
    "        #normalize each element of the lists\n",
    "        x = 0\n",
    "        while x < len(data[i]):\n",
    "            # 0-1 normalisation\n",
    "            # divide each element by the max value for that attribute in set\n",
    "            data[i][x] = data[i][x] / max_values_array[x];\n",
    "            x = x + 1;\n",
    "    \n",
    "    return np.asarray(data);\n",
    " \n",
    "    \n",
    "# softmax\n",
    "#\n",
    "# Logisitc Regression function used to restrict the outputs to a \n",
    "# range of 0 --> 1. The outputs can then be interpretted as a \n",
    "# probability or odds.\n",
    "#\n",
    "# @params {list} x - data to train/test on \n",
    "def softmax(x):\n",
    "    # e = numpy.exp(x - numpy.max(x))  # prevent overflow\n",
    "    e = numpy.exp(x)\n",
    "    # when there is only 1 instance to test\n",
    "    if e.ndim == 1:\n",
    "        return e / numpy.sum(e, axis=0)\n",
    "    # testing multiple instances\n",
    "    else:  \n",
    "        return e / numpy.array([numpy.sum(e, axis=1)]).T  # number of dimensions = 4\n",
    "\n",
    "# Percentage Split Data\n",
    "#\n",
    "# Split the data by a given percentage based on whether you are training \n",
    "# or testing the model\n",
    "#\n",
    "# object - contains the parameters that define the Percentage Split Data Class\n",
    "def percentage_split_data(data, locations, percentage):\n",
    "    sample_locations = locations;\n",
    "    \n",
    "    if percentage: \n",
    "        # get the number of samples needed to make up specified percentage\n",
    "        number_of_samples = int( len(data)*(percentage/100) )\n",
    "        # list of indices to sample the data with\n",
    "        sample_locations = rnd.sample(range(len(data)), number_of_samples)\n",
    "        \n",
    "    split_list = []\n",
    "    for index, row in enumerate(data):\n",
    "        # if the index is found in the list of random indices, take the instance\n",
    "        if index in sample_locations:\n",
    "            split_list.append(row)\n",
    "\n",
    "    return split_list;\n",
    "\n",
    "# convert_predictions_binary\n",
    "#\n",
    "# Convert the highest predicted probabilities to 1\n",
    "# otherwise set as 0\n",
    "#\n",
    "# @params {list} results - results from testing model\n",
    "def convert_predictions_binary(results):\n",
    "    converted_rersult = []\n",
    "    for row in results:\n",
    "        # create array of zeros for the number of classes\n",
    "        zeros = [0] * number_of_classes;\n",
    "        # find the max probability of the set\n",
    "        for index, el in enumerate(row):\n",
    "            if el == max(row):\n",
    "                index_of_max = index;\n",
    "        zeros[index_of_max] = 1;\n",
    "        \n",
    "        converted_rersult.append(zeros);\n",
    "                \n",
    "    return converted_rersult;\n",
    "\n",
    "# create_output_string\n",
    "#\n",
    "# Create maniplative string to write to file\n",
    "# Makes it easier to read back the results\n",
    "#\n",
    "# @params {list} data - results from testing model\n",
    "def create_output_string(data):\n",
    "    # create string to save to output\n",
    "    output_string = ''\n",
    "    for index1, test in enumerate(data):\n",
    "        string = ''\n",
    "        for index2, val in enumerate(test):\n",
    "            int(val)\n",
    "            string += str(int(val))\n",
    "            if index2 < len(data[0])-1:\n",
    "                string += ','\n",
    "\n",
    "        output_string += string\n",
    "        output_string += '|'\n",
    "    \n",
    "    return output_string;\n",
    "\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Class and Test Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "##################################### REGRESSION CLASS ######################################\n",
    "\n",
    "# Logistic Regression\n",
    "#\n",
    "# Inner class to build the Logistic Regression model.\n",
    "# It trains the data on 66% and tests on 33%.\n",
    "#\n",
    "# object - contains the parameters that define the Logistic Regression Class\n",
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, type_of_owl, n_in, n_out):\n",
    "        self.x = input                       # input values of owls\n",
    "        self.y = type_of_owl                 # output values \n",
    "        self.W = numpy.zeros((n_in, n_out))  # initialize W 0\n",
    "        self.b = numpy.zeros(n_out)          # initialize bias 0\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    # train the model\n",
    "    #Â lr = learning rate\n",
    "    def train_model(self, lr=0.015, input=None, L2_reg=0.00):\n",
    "        if input is not None:\n",
    "            self.x = input\n",
    "\n",
    "        # find the predicted probabilties by running the sum of vector product of the input data\n",
    "        # & array of zeros and an array of zeros.\n",
    "        # on each iteration the value of self.W and self.b are calculated below\n",
    "        prob_y_given_x = softmax(numpy.dot(self.x, self.W) + self.b)\n",
    "        # calculate the difference in the predited probability and the actual value\n",
    "        diff_y = self.y - prob_y_given_x\n",
    "        # append to the value of self.W and self.B on every iteration\n",
    "        self.W += (lr * numpy.dot(self.x.T, diff_y)) - (lr * L2_reg * self.W)\n",
    "        # multiply the learning rate by the mean of the difference in the actual vs. prediction\n",
    "        self.b += lr * numpy.mean(diff_y, axis=0)\n",
    "    \n",
    "    \n",
    "    # test the model\n",
    "    def predict_probabilities(self, x):\n",
    "        return softmax(numpy.dot(x, self.W) + self.b)\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "####################################### TEST FUNCTION ########################################\n",
    "def test_algorithm(learning_rate=0.015, n_interations=1000):\n",
    "    training_percentage = 66\n",
    "    testing_percentage = 100 - training_percentage\n",
    "    \n",
    "    # training data\n",
    "    input_data = numpy.array( file_reader('../data/input-data') );\n",
    "    # get the number of samples needed to make up specified percentage\n",
    "    number_of_samples = int( len(input_data)*(training_percentage/100) )\n",
    "    # list of indices to sample the data with\n",
    "    sample_locations = rnd.sample(range(len(input_data)), number_of_samples)\n",
    "    \n",
    "    # get percentage of dataset for training\n",
    "    sampled_input_data = percentage_split_data(input_data, sample_locations, None)\n",
    "    # normalize the data\n",
    "    x = normalise_data(sampled_input_data)\n",
    "    \n",
    "    # types of owl\n",
    "    owl_types = numpy.array( file_reader('../data/owl-types') );\n",
    "    # outputs: (data, percentage split)\n",
    "    y = percentage_split_data(owl_types, sample_locations, None)\n",
    "\n",
    "    # build LogisticRegression model\n",
    "    LogisticRegressionModel = LogisticRegression(input=x, type_of_owl=y, n_in=(len(df.columns)-1), n_out=number_of_classes)\n",
    "\n",
    "    #### TRAIN MODEL ####\n",
    "    for iteration in range(n_interations):\n",
    "        LogisticRegressionModel.train_model(lr=learning_rate)\n",
    "        learning_rate *= 0.96\n",
    "                \n",
    "\n",
    "    #### TEST MODEL ####\n",
    "    # get the locations of the samples not used in the training of the model\n",
    "    # and use them for testing the model. \n",
    "    test_sample_locations = []\n",
    "    for location in range(135):\n",
    "        if location not in sample_locations:\n",
    "            test_sample_locations.append(location);\n",
    "\n",
    "    # sample the data for testing\n",
    "    testing_data_x = percentage_split_data(input_data, test_sample_locations, None)\n",
    "    # normalize the data\n",
    "    normalised_testing_data_x = normalise_data(testing_data_x)\n",
    "    # run the prediction model on the data\n",
    "    prediction_model = LogisticRegressionModel.predict_probabilities(normalised_testing_data_x)\n",
    "    # convert the highst probabiility to a 1 and the others to a 0\n",
    "    probabilities = convert_predictions_binary(prediction_model)\n",
    "\n",
    "    # compare the results to the actual value of the \n",
    "    testing_data_y = percentage_split_data(owl_types, test_sample_locations, None)\n",
    "    \n",
    "    \n",
    "    output_string = create_output_string(probabilities)\n",
    "    output_string = output_string[:-1]\n",
    "    with open('../data/results/predictions', 'a') as predictions:\n",
    "        predictions.write(output_string)\n",
    "        predictions.write('\\n')\n",
    "        \n",
    "    output_string = create_output_string(testing_data_y)\n",
    "    output_string = output_string[:-1]\n",
    "    with open('../data/results/actual-probabilities', 'a') as predictions:\n",
    "        predictions.write(output_string)\n",
    "        predictions.write('\\n')\n",
    "    \n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Built\n",
      "Predictions Made\n"
     ]
    }
   ],
   "source": [
    "####################################### MAIN PROGRAM ########################################\n",
    "if __name__ == \"__main__\":\n",
    "    test_algorithm()\n",
    "    print('Model Built\\nPredictions Made');\n",
    "#############################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
